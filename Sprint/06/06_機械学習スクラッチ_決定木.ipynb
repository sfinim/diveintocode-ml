{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定木学習とは\n",
    "決定木学習は 決定木 と呼ばれる 木構造のグラフ を作る機械学習手法です。機械学習の分野では学習手法も単に「決定木」と呼ばれます。\n",
    "\n",
    "分類にも回帰にも使え、分類の場合3クラス以上の多値分類が可能です。ここでは基本となる分類のみを扱います。\n",
    "\n",
    "#### 決定木とは\n",
    "決定木は、属性 と 値 の組｛属性1：値1，属性2：値2, 属性3：値3,…，属性n：値n｝によって表現されたデータを、条件分岐を繰り返すことであるクラスに割り当てることができる木構造のグラフです。\n",
    "\n",
    "以下の例は会場の気温という属性の値によって、開催と中止のクラスに割り当てるグラフです。「会場の気温という属性の値は35以上かどうか」という条件分岐1回による決定木による分類が行えます。例えば36度がこの決定木にインプットされれば、中止というアウトプット（判断）ができます。\n",
    "\n",
    "<img src=\"https://t.gyazo.com/teams/diveintocode/ca1760b9db2eff08bc82102db1bf7eea.png\" width=\"400\">\n",
    "\n",
    "なお、「属性と値」は機械学習の分野では「特徴量の名前と特徴量の値」のことです。これ以降は単に特徴量という呼びます。\n",
    "\n",
    "#### 各種用語\n",
    "もう少し複雑な例で決定木で重要な用語を確認します。特徴量が「雨量」「屋内かどうか」「風の強さ」の3種類で、イベントの開催か中止かを分類する場合で考えてみます。訓練データを学習することで、以下のような決定木が作れます。\n",
    "\n",
    "<img src=\"https://t.gyazo.com/teams/diveintocode/c927a798dc2292cc05663301dde78632.png\" width=\"400\">\n",
    "\n",
    "丸で囲われたひとつひとつを ノード と呼びます。ノードには親子関係を考えることができ、例えば(0)のノードは(1)(2)(3)のノードの 親ノード と呼びます。逆に、(1)(2)(3)のノードは(0)のノードの 子ノード と呼びます。\n",
    "\n",
    "一番上の(0)は 根ノード 、 末端の(1)(4)(5)(7)(8)(9)のような分類結果を表すノードは 葉ノード と呼びます。\n",
    "\n",
    "条件分岐の矢印は エッジ と呼びます。あるノードから根ノードまでのエッジの数が 深さ です。(3)の深さは1、(6)の深さは2、(9)の深さは3という風になります。この決定木の最大の深さは3です。\n",
    "\n",
    "これは(0)に対して(1)(2)(3)の3つのノードが分かれている多岐分岐の決定木ですが、機械学習では2つにしか分かれないものが一般的です。学習時の複雑さを減らすためです。\n",
    "\n",
    "#### どう決定木を作るか\n",
    "決定木の学習には様々なやり方が存在しますが、その中のある方法についてスクラッチを行いながら見ていきます。\n",
    "\n",
    "学習方法やハイパーパラメータ、訓練データ次第で作られる決定木は異なってきます。\n",
    "\n",
    "#### 推定を考える\n",
    "以下の場合、イベントは開催されるでしょうか。決定木を使って判断してください。\n",
    "\n",
    "|雨量[mm]|屋内かどうか|風の強さ[m/s]|\n",
    "|---|---|---|\n",
    "|2.5|1（屋内）|5|\n",
    "\n",
    "答えは「開催」です。以下の赤線の順でたどっていきます。\n",
    "\n",
    "<img src=\"https://t.gyazo.com/teams/diveintocode/3abf4302fd28b9c58e9c4f86e5878661.png\" width=\"400\">\n",
    "\n",
    "これが決定木による推定の操作になります。\n",
    "\n",
    "#### 扱える特徴量\n",
    "決定木は理論上は量的変数だけでなく、カテゴリ変数も扱えます。しかし、scikit-learnの実装では量的変数のみに対応していますので、スクラッチ実装もそのように作成します。上記の例ですと「会場の種類」で「屋内と屋外」ですとカテゴリ変数ですが、「屋内かどうか」で「0と1」と量的変数にすることで扱えるようにしています。\n",
    "\n",
    "## 決定木スクラッチ\n",
    "分類のための決定木のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "決定木の学習には何回まで条件分岐を繰り返すかを表す （最大の）深さ というハイパーパラメータが登場しますが、深さ1の実装を必須課題とします。深さが2以上のものはアドバンス課題とします。\n",
    "\n",
    "学習の仕方には様々な方法がありますが、ここではscikit-learnでも使用されている CART法 をベースとした実装を行います。この方法では学習の複雑さを減らすために、 分岐は2つに分かれるのみ になります。\n",
    "\n",
    "以下に雛形を用意してあります。このScratchDecesionTreeClassifierDepth1クラスにコードを書き加えていってください。\n",
    "\n",
    "《雛形》\n",
    "```\n",
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    深さ1の決定木分類器のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        決定木分類器を学習する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        決定木分類器を使いラベルを推定する\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分割の条件を学習で求める\n",
    "学習によって、ノードをどういった条件で分割すると、うまく分けられるかということを求めます。\n",
    "\n",
    "うまく分けられていることを判定するためにノードに対してジニ不純度と情報利得という値を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDecesionTreeClassifierDepth1():\n",
    "    \"\"\"\n",
    "    深さ1の決定木分類器のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.verbose = verbose\n",
    "        self.list_node = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        決定木分類器を学習する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        決定木分類器を使いラベルを推定する\n",
    "        \"\"\"\n",
    "        pass\n",
    "        return\n",
    "    \n",
    "    def _ginis_diversity_index(self, dict_class_sample):\n",
    "        \"\"\"\n",
    "        ジニ係数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dict_class_sample : クラスとサンプル数を紐付けた辞書型\n",
    "        keyがクラス、valueが各クラスのサンプル数\n",
    "        \"\"\"\n",
    "        N = sum(dict_class_sample.values())\n",
    "\n",
    "        tmp_sum = 0\n",
    "        for i in dict_class_sample.keys():\n",
    "            tmp_sum += (dict_class_sample[i]/N)**2\n",
    "        I_t = 1 - tmp_sum\n",
    "        \n",
    "        return I_t\n",
    "    \n",
    "    def _information_gain(self, dict_class_sample_left, dict_class_sample_right):\n",
    "        \"\"\"\n",
    "        情報利得を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dict_class_sample_left : クラスとサンプル数を紐付けた辞書型(左ノード)\n",
    "        dict_class_sample_right : クラスとサンプル数を紐付けた辞書型(右ノード)\n",
    "        keyがクラス、valueが各クラスのサンプル数\n",
    "        \"\"\"\n",
    "        p_samples = np.array(list(dict_class_sample_left.values())) + np.array(list(dict_class_sample_right.values()))\n",
    "        dict_class_sample_p = dict(zip(list(dict_class_sample_left.keys()), p_samples)) \n",
    "\n",
    "        N_p_all = sum(list(dict_class_sample_p.values()))\n",
    "        N_left_all = sum(list(dict_class_sample_left.values()))\n",
    "        N_right_all = sum(list(dict_class_sample_right.values()))\n",
    "\n",
    "        IG = _ginis_diversity_index(dict_class_sample_p) \\\n",
    "             - (N_left_all/N_p_all)*_ginis_diversity_index(dict_class_sample_left) \\\n",
    "             - (N_right_all/N_p_all)*_ginis_diversity_index(dict_class_sample_right)\n",
    "\n",
    "        return IG\n",
    "    \n",
    "\n",
    "class ScratchDecesionTreeNode():\n",
    "    \"\"\"\n",
    "    ScratchDecesionTreeClassifierDepth1で使用するノード情報\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node_id : int\n",
    "      ノードID\n",
    "      \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.p_node_id : int\n",
    "      親ノードID\n",
    "    self.right_node_id : int\n",
    "      右ノードID(-1の場合は無しを意味する)\n",
    "    self.left_node_id : int\n",
    "      左ノードID(-1の場合は無しを意味する)\n",
    "    self.class_ : int\n",
    "      葉ノードとなったときに記録されるクラス(-1の場合は葉ノードではないことを意味する)\n",
    "    \"\"\"\n",
    "    def __init__(self, node_id):\n",
    "        if node_id < 0:\n",
    "            print(\"エラー\")\n",
    "            return\n",
    "        self.node_id = node_id\n",
    "        self.right_node_id = -1\n",
    "        self.left_node_id = -1\n",
    "        self.class_ = -1\n",
    "        \n",
    "    def set_right_node_id(self, right_node_id):\n",
    "        self.right_node_id = right_node_id\n",
    "        \n",
    "    def get_right_node_id(self):\n",
    "        return self.right_node_id\n",
    "    \n",
    "    def set_left_node_id(self, left_node_id):\n",
    "        self.left_node_id = left_node_id\n",
    "        \n",
    "    def get_left_node_id(self):\n",
    "        return self.left_node_id\n",
    "    \n",
    "    def set_class(self, class_):\n",
    "        self.class_ = class_\n",
    "        \n",
    "    def get_class(self):\n",
    "        return self.class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】不純度を求める関数\n",
    "ノード の ジニ不純度 を計算する関数を作成してください。ノード $t$ に対するジニ不純度 $I(t)$ は以下の数式で求まります。クラスが混じり合っているほどジニ不純度は高くなります。\n",
    "\n",
    "$$\n",
    "I(t)=1-\\sum_{i=1}^KP^2(C_i|t)=1-\\sum_{i=1}^K(\\frac{N_{t,i}}{N_{t,all}})^2\n",
    "$$\n",
    "\n",
    "$t$ : ノードのインデックス\n",
    "\n",
    "$i$ : クラスのインデックス\n",
    "\n",
    "$K$ : クラスの数\n",
    "\n",
    "$C_i$ : i番目のクラス\n",
    "\n",
    "$P(C_i|t)$ :　t番目のノードにおける$C_i$の割合\n",
    "\n",
    "$N_{t,i}$ : t番目のノードのi番目のクラスに属するサンプル数\n",
    "\n",
    "$N_{t,all}$ : t番目のノードのサンプルの総数\n",
    "\n",
    "まずは簡単な例を作り、手計算と関数の結果を比較してください。\n",
    "\n",
    "《例》\n",
    "\n",
    "クラス1:サンプル数15, クラス2:サンプル数15 → ジニ不純度0.500\n",
    "\n",
    "クラス1:サンプル数15, クラス2:サンプル数15, クラス3:サンプル数15 → ジニ不純度0.667\n",
    "\n",
    "クラス1:サンプル数18, クラス2:サンプル数12 → ジニ不純度0.480\n",
    "\n",
    "クラス1:サンプル数30, クラス2:サンプル数0 → ジニ不純度0.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 引数の決定\n",
    "  - クラスとサンプル数を紐付けた辞書型(dict_class_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検算1\n",
    "dict_class_sample = {1: 15, 2: 15}\n",
    "\n",
    "N = sum(dict_class_sample.values())\n",
    "\n",
    "tmp_sum = 0\n",
    "for i in dict_class_sample.keys():\n",
    "    tmp_sum += (dict_class_sample[i]/N)**2\n",
    "I_t = 1 - tmp_sum\n",
    "I_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検算2\n",
    "dict_class_sample = {1: 15, 2: 15, 3: 15}\n",
    "\n",
    "N = sum(dict_class_sample.values())\n",
    "\n",
    "tmp_sum = 0\n",
    "for i in dict_class_sample.keys():\n",
    "    tmp_sum += (dict_class_sample[i]/N)**2\n",
    "I_t = 1 - tmp_sum\n",
    "I_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検算3\n",
    "dict_class_sample = {1: 18, 2: 12}\n",
    "\n",
    "N = sum(dict_class_sample.values())\n",
    "\n",
    "tmp_sum = 0\n",
    "for i in dict_class_sample.keys():\n",
    "    tmp_sum += (dict_class_sample[i]/N)**2\n",
    "I_t = 1 - tmp_sum\n",
    "I_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検算4\n",
    "dict_class_sample = {1: 30, 2: 0}\n",
    "\n",
    "N = sum(dict_class_sample.values())\n",
    "\n",
    "tmp_sum = 0\n",
    "for i in dict_class_sample.keys():\n",
    "    tmp_sum += (dict_class_sample[i]/N)**2\n",
    "I_t = 1 - tmp_sum\n",
    "I_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _ginis_diversity_indexを追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】情報利得を求める関数\n",
    "次に、ノード間の 情報利得 を計算する関数を作成してください。問題1で作成したジニ不純度 $I(t)$ を計算する関数を呼び出して使います。情報利得$IG$は以下の数式で求まります。うまく分けられている時ほど情報利得は大きくなります。\n",
    "\n",
    "ここで分岐は2つのみであるため、分岐先を「左側のノード・右側のノード」と呼びます。\n",
    "\n",
    "$$\n",
    "IG(p)=I(p)-\\frac{N_{left,all}}{N_{p,all}}I(left)-\\frac{N_{right,all}}{N_{p,all}}I(right)\n",
    "$$\n",
    "\n",
    "$p$ : 親ノードを示すインデックス\n",
    "\n",
    "$left$ : 左側のノードを示すインデックス\n",
    "\n",
    "$right$ : 右側のノードを示すインデックス\n",
    "\n",
    "まずは簡単な例を作り、手計算と関数の結果を比較してください。\n",
    "\n",
    "《例》\n",
    "\n",
    "左ノードクラス1:サンプル数10, 左ノードクラス2:サンプル数30, 右ノードクラス1:サンプル数20, 右ノードクラス2:サンプル数5 → 情報利得0.143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 引数の決定\n",
    "  - クラスとサンプル数を紐付けた辞書型(左ノード)(dict_class_sample_left)\n",
    "  - クラスとサンプル数を紐付けた辞書型(右ノード)(dict_class_sample_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ginis_diversity_index(dict_class_sample):\n",
    "    \"\"\"\n",
    "    ジニ係数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_class_sample : クラスとサンプルのインデックスを紐付けた辞書型\n",
    "    keyがクラス、valueが各クラスのサンプルのndarray\n",
    "    \"\"\"\n",
    "    N = sum(dict_class_sample.values())\n",
    "    tmp_sum = 0\n",
    "    for i in dict_class_sample.keys():\n",
    "        tmp_sum += (dict_class_sample[i]/N)**2\n",
    "    I_t = 1 - tmp_sum\n",
    "    \n",
    "    return I_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14319526627218937"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_class_sample_left = {1: 10, 2: 30}\n",
    "dict_class_sample_right = {1: 20, 2: 5}\n",
    "\n",
    "p_samples = np.array(list(dict_class_sample_left.values())) + np.array(list(dict_class_sample_right.values()))\n",
    "dict_class_sample_p = dict(zip(list(dict_class_sample_left.keys()), p_samples)) \n",
    "\n",
    "N_p_all = sum(list(dict_class_sample_p.values()))\n",
    "N_left_all = sum(list(dict_class_sample_left.values()))\n",
    "N_right_all = sum(list(dict_class_sample_right.values()))\n",
    "\n",
    "IG = _ginis_diversity_index(dict_class_sample_p) \\\n",
    "     - (N_left_all/N_p_all)*_ginis_diversity_index(dict_class_sample_left) \\\n",
    "     - (N_right_all/N_p_all)*_ginis_diversity_index(dict_class_sample_right)\n",
    "\n",
    "IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(dict_class_sample_left.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _information_gainを追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】学習\n",
    "空間の分割を行い、決定木のグラフを生成するコードを作成してください。今は深さ1の決定木なので、分割を1回だけ行います。ここでグラフを生成するとは、1回の分割の際の条件としてどの特徴量がいくつ以上の時とするかを求めるということです。\n",
    "\n",
    "訓練データに対して全ての組み合わせの分割を行い、その中でノード間の情報利得が最大となる分割をそのノードの分割基準として記録します。\n",
    "\n",
    "クラスが混ざらない不純度が0のノード、または指定された深さのノードが 葉ノード となります。葉ノードにはクラスを記録しておき、これを推定時に分類するクラスとします。クラスが混ざらない場合はそのままのクラスを記録し、混ざっている場合は多数決により決めます。\n",
    "\n",
    "《組み合わせの取り方》\n",
    "\n",
    "全ての組み合わせの取り方は、最も単純には各特徴量の値自体をしきい値にして分割を行う方法があります。片側の端は今回のスクラッチはこの方法で行なってください。\n",
    "\n",
    "他には中間の値をしきい値にする方法もあり、scikit-learnではこの方法が用いられています。\n",
    "\n",
    "《補足》\n",
    "\n",
    "問題2の情報利得を計算する関数はこの問題3で利用する上では、親ノードの不純度 $I(p)$ は固定されるため、左右のノードの不純度の合計を計算するだけでも同じ結果が得られることになります。しかし、ここでは親ノードを考慮した情報利得を計算する実装を行なってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アルゴリズム\n",
    "- 以下を、サンプル数分繰り返す\n",
    "    - あるサンプルの特徴量をthresholdに設定する\n",
    "    - thresholdを基準にサンプルを2クラスに分割する\n",
    "    - IGを求め、list_IGに設定\n",
    "- max(list_IG)のthresholdを記録しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _information_gain(dict_class_sample_left, dict_class_sample_right):\n",
    "    \"\"\"\n",
    "    情報利得を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_class_sample_left : クラスとサンプル数を紐付けた辞書型(左ノード)\n",
    "    dict_class_sample_right : クラスとサンプル数を紐付けた辞書型(右ノード)\n",
    "    keyがクラス、valueが各クラスのサンプル数\n",
    "    \"\"\"\n",
    "    p_samples = np.array(list(dict_class_sample_left.values())) + np.array(list(dict_class_sample_right.values()))\n",
    "    dict_class_sample_p = dict(zip(list(dict_class_sample_left.keys()), p_samples)) \n",
    "\n",
    "    N_p_all = sum(list(dict_class_sample_p.values()))\n",
    "    N_left_all = sum(list(dict_class_sample_left.values()))\n",
    "    N_right_all = sum(list(dict_class_sample_right.values()))\n",
    "\n",
    "    rate_left = N_left_all/N_p_all\n",
    "    rate_right = N_right_all/N_p_all\n",
    "\n",
    "    if rate_left != 0:\n",
    "        gini_left = _ginis_diversity_index(dict_class_sample_left)\n",
    "    else:\n",
    "        gini_left = 0\n",
    "    \n",
    "    if rate_right != 0:\n",
    "        gini_right = _ginis_diversity_index(dict_class_sample_right)\n",
    "    else:\n",
    "        gini_right = 0\n",
    "\n",
    "    IG = _ginis_diversity_index(dict_class_sample_p) - rate_left*gini_left - rate_right*gini_right\n",
    "\n",
    "    return IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,4,6],[4,5,6],[10,12,13],[3,4,5],[12,14,15]])\n",
    "y = np.array([1,1,2,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  7, 10, 13])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 2} {1: 1, 2: 0}\n",
      "{1: 1, 2: 2} {1: 2, 2: 0}\n",
      "{1: 0, 2: 2} {1: 3, 2: 0}\n",
      "{1: 0, 2: 1} {1: 3, 2: 1}\n",
      "{1: 0, 2: 0} {1: 3, 2: 2}\n",
      "{1: 1, 2: 2} {1: 2, 2: 0}\n",
      "{1: 0, 2: 2} {1: 3, 2: 0}\n",
      "{1: 0, 2: 1} {1: 3, 2: 1}\n",
      "{1: 0, 2: 0} {1: 3, 2: 2}\n",
      "{1: 2, 2: 2} {1: 1, 2: 0}\n",
      "{1: 0, 2: 2} {1: 3, 2: 0}\n",
      "{1: 0, 2: 1} {1: 3, 2: 1}\n",
      "{1: 0, 2: 0} {1: 3, 2: 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.07999999999999996, 0.21333333333333332, 0.48, 0.17999999999999994, 0.0],\n",
       " [0.21333333333333332, 0.48, 0.17999999999999994, 0.0],\n",
       " [0.07999999999999996, 0.48, 0.17999999999999994, 0.0]]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_class_sample_left = {}\n",
    "dict_class_sample_right = {}\n",
    "\n",
    "list_info_gain = []\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    features = np.unique(X[:, i])\n",
    "    labels = np.unique(y)\n",
    "\n",
    "    info_gain = []\n",
    "    for threshold in features:\n",
    "        sample_left = X[X[:, i] > threshold]\n",
    "        label_left = y[X[:, i] > threshold]\n",
    "        sample_right = X[X[:, i] <= threshold]\n",
    "        label_right = y[X[:, i] <= threshold]\n",
    "        \n",
    "        tmp = np.where(label_left == labels[0], True, False)\n",
    "        dict_class_sample_left[labels[0]] = len(sample_left[tmp])\n",
    "        tmp = np.where(label_left == labels[1], True, False)        \n",
    "        dict_class_sample_left[labels[1]] = len(sample_left[tmp])\n",
    "        \n",
    "        tmp = np.where(label_right == labels[0], True, False)\n",
    "        dict_class_sample_right[labels[0]] = len(sample_right[tmp])\n",
    "        tmp = np.where(label_right == labels[1], True, False)        \n",
    "        dict_class_sample_right[labels[1]] = len(sample_right[tmp])\n",
    "        \n",
    "        print(dict_class_sample_left, dict_class_sample_right)\n",
    "        \n",
    "        info_gain.append(_information_gain(dict_class_sample_left, dict_class_sample_right))\n",
    "     \n",
    "    list_info_gain.append(info_gain)\n",
    "\n",
    "list_info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 12 13]\n",
      " [12 14 15]]\n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "print(X[X[:, 0] > 4])\n",
    "print(y[X[:, 0] > 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.where(y[X[:, 0] > 4] == 1, True, False)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 8, 9]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X[:, 0] > 4][tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True,  True])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logical_not(np.where(y == 1, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】推定\n",
    "推定する仕組みを実装してください。ScratchDecesionTreeClassifierDepth1クラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "入力されたデータの値を学習した条件で判定していき、どの葉ノードに到達するかを見ます。葉ノードにはクラスが記録されているので、これが推定値となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したシンプルデータセット2の2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】決定領域の可視化\n",
    "決定領域を可視化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）深さ2の決定木分類器クラスの作成\n",
    "深さが2の決定木分類器のクラスScratchDecesionTreeClassifierDepth2を作成してください。\n",
    "\n",
    "深さ2とは空間の分割を2回行うことを指します。\n",
    "\n",
    "《ヒント》\n",
    "\n",
    "各ノードをインスタンスとして扱うと、任意の深さへの拡張が行いやすくなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】（アドバンス課題）深さに制限のない決定木分類器クラスの作成\n",
    "深さに制限のない決定木分類器のクラスScratchDecesionTreeClassifierDepthInfを作成してください。\n",
    "\n",
    "任意の深さを指定できるようにするとともに、指定しない場合は全ての葉ノードがジニ不純度0となるまで続けられるようにもしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
