{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1次元の畳み込みニューラルネットワークスクラッチ\n",
    "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
    "\n",
    "#### 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\n",
    "#### データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import cupy as np\n",
    "#import chainer.cuda\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# データの読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# ラベルをone-hot化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# 分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "\n",
    "# cupy用\n",
    "# X_train = np.array(X_train)\n",
    "# X_val = np.array(X_val)\n",
    "# y_train = np.array(y_train)\n",
    "# y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "$$\n",
    "w_s'=w_s-\\alpha\\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b'=b-\\alpha\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s}=\\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}x_{(1+s)} \\\\\n",
    "\\frac{\\partial L}{\\partial b}=\\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j}=\\sum_{s=0}^{F-1}\\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_info:(filter_num, filtersize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    def W(self, filter_info):\n",
    "        filter_num, filtersize = filter_info\n",
    "        self.sigma = 1 / np.sqrt(filter_num)\n",
    "        W = self.sigma * np.random.randn(filter_num, filtersize)\n",
    "        return W\n",
    "    def B(self, filter_info):\n",
    "        filter_num, filtersize = filter_info\n",
    "        B = np.zeros(filter_num)\n",
    "        return B\n",
    "\n",
    "\n",
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    def W(self, filter_info):\n",
    "        filter_num, filtersize = filter_info\n",
    "        self.sigma = np.sqrt(2 / filter_num)\n",
    "        W = self.sigma * np.random.randn(filter_num, filtersize)\n",
    "        return W\n",
    "    def B(self, filter_info):\n",
    "        filter_num, filtersize = filter_info\n",
    "        B = np.zeros(filter_num)\n",
    "        return B\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.b -= self.lr * layer.db\n",
    "        \n",
    "        \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.h_W = None\n",
    "        self.h_b = None\n",
    "    def update(self, layer):\n",
    "        if (self.h_W is None) and (self.h_b is None):\n",
    "            self.h_W = 0\n",
    "            self.h_b = 0\n",
    "        \n",
    "        self.h_W += (layer.dW ** 2).sum()\n",
    "        self.h_b += (layer.db ** 2).sum()\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + 1e-7)\n",
    "        layer.b -= self.lr * layer.db / (np.sqrt(self.h_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    def __init__(self, filter_info, initializer, optimizer):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_info)\n",
    "        self.b = initializer.B(filter_info)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, filtersize = self.W.shape\n",
    "        self.X = X\n",
    "        # フィルターと内積を取る全ての要素(col)を導出\n",
    "        out_width = X.shape[1] - filtersize + 1\n",
    "        col = np.zeros((X.shape[0], filtersize, out_width))\n",
    "\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            col[:, width, :] = X[:, width:width_max]\n",
    "        \n",
    "        col = col.transpose(0, 2, 1).reshape(X.shape[0]*out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        \n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        \n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        out_width = self.X.shape[1] - filtersize + 1\n",
    "        dcol = dcol.reshape(self.X.shape[0], out_width, filtersize).transpose(0, 2, 1)\n",
    "        \n",
    "        dX = np.zeros(self.X.shape)\n",
    "        \n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            dX[:, width:width_max] += dcol[:, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.46941314, -5.97876848, -6.08885051, ...,  0.23189721,\n",
       "        -0.16132301, -0.47157834],\n",
       "       [-0.23470657, -3.45879738, -9.02319373, ...,  0.38082953,\n",
       "        -1.10447968, -0.47157834],\n",
       "       [-0.23470657, -2.98938424, -2.34030554, ..., -1.40695109,\n",
       "         0.32264602,  0.94315667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_info = (1, 780)\n",
    "initializer = HeInitializer()\n",
    "optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "conv = SimpleConv1d(filter_info, initializer, optimizer)\n",
    "out = conv.forward(X_train[:3])\n",
    "print(out.shape)\n",
    "conv.backward(np.array([[[-2, -1, -3, -1, 2],[-2, -1, -3, -1, 2],[-2, -1, -3, -1, 2]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "$$\n",
    "N_{out}=\\frac{N_{im}+2P-F}{S}+1\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_size(N_in, paddingSize, filterSize, strideSize):\n",
    "    N_out = (N_in + 2*paddingSize - filterSize)//strideSize + 1\n",
    "    return N_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output_size(10, 0, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output_size(10, 1, 8, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。\n",
    "```\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "```\n",
    "\n",
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "```\n",
    "a = np.array([35, 50])\n",
    "```\n",
    "\n",
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "```\n",
    "delta_a = np.array([10, 20])\n",
    "```\n",
    "\n",
    "バックプロパゲーションをすると次のような値になります。\n",
    "```\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])\n",
    "```\n",
    "\n",
    "#### 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "$$\n",
    "a_i=\\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "$$\n",
    "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
    "$$\n",
    "\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
    "```\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "a = a.sum(axis=1)\n",
    "```\n",
    "\n",
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
    "```\n",
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
    "```\n",
    "\n",
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "《参考》\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "[Indexing — NumPy v1.17 Manual](https://numpy.org/doc/stable/reference/arrays.indexing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d_test:\n",
    "    def __init__(self, filter_info, initializer, optimizer):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = np.array([[3, 5, 7]]).astype(float)\n",
    "        self.b = np.array([[1]]).astype(float)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, filtersize = self.W.shape\n",
    "        self.X = X\n",
    "        # フィルターと内積を取る全ての要素(col)を導出\n",
    "        out_width = X.shape[1] - filtersize + 1\n",
    "        col = np.zeros((X.shape[0], filtersize, out_width))\n",
    "\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            col[:, width, :] = X[:, width:width_max]\n",
    "        \n",
    "        col = col.transpose(0, 2, 1).reshape(X.shape[0]*out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        \n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        out_width = self.X.shape[1] - filtersize + 1\n",
    "        dcol = dcol.reshape(self.X.shape[0], out_width, filtersize).transpose(0, 2, 1)\n",
    "        dX = np.zeros(self.X.shape)\n",
    "        \n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            dX[:, width:width_max] += dcol[:, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[35. 50.]]]\n",
      "[30]\n",
      "[[ 50.  80. 110.]]\n",
      "[[ 30. 110. 170. 140.]]\n"
     ]
    }
   ],
   "source": [
    "test_filter_info = (5, 700)\n",
    "test_initializer = HeInitializer()\n",
    "test_optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "test_conv = SimpleConv1d_test(test_filter_info, test_initializer, test_optimizer)\n",
    "out = test_conv.forward(np.array([[1,2,3,4]]))\n",
    "print(out)\n",
    "dout = test_conv.backward(np.array([[[10, 20]]]))\n",
    "print(test_conv.db)\n",
    "print(test_conv.dW)\n",
    "print(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "```\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "```\n",
    "\n",
    "出力は次のようになります。\n",
    "```\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
    "```\n",
    "\n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "《補足》\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_info:(filter_num, channel, filtersize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, filter_info, initializer, optimizer):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_info)\n",
    "        self.b = initializer.B(filter_info)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        self.X = X\n",
    "        # フィルターと内積を取る全ての要素(col)を導出\n",
    "        out_width = X.shape[2] - filtersize + 1\n",
    "        col = np.zeros((X.shape[0], channel_in, filtersize, out_width))\n",
    "\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            col[:, :, width, :] = X[:, :, width:width_max]\n",
    "        \n",
    "        col = col.transpose(0, 1, 3, 2).reshape(X.shape[0]*out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, channel_in, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        out_width = self.X.shape[2] - filtersize + 1\n",
    "        dcol = dcol.reshape(self.X.shape[0], out_width, channel_in, filtersize).transpose(0, 2, 3, 1)\n",
    "        dX = np.zeros(self.X.shape)\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            dX[:, :, width:width_max] += dcol[:, :, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_test:\n",
    "    def __init__(self, filter_info, initializer, optimizer):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = np.ones((3, 2, 3)).astype(float)\n",
    "        self.b = np.array([1, 2, 3]).astype(float)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        self.X = X\n",
    "        # フィルターと内積を取る全ての要素(col)を導出\n",
    "        out_width = X.shape[2] - filtersize + 1\n",
    "        col = np.zeros((X.shape[0], channel_in, filtersize, out_width))\n",
    "\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            col[:, :, width, :] = X[:, :, width:width_max]\n",
    "        \n",
    "        col = col.transpose(0, 1, 3, 2).reshape(X.shape[0]*out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, channel_in, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        out_width = self.X.shape[2] - filtersize + 1\n",
    "        dcol = dcol.reshape(self.X.shape[0], out_width, channel_in, filtersize).transpose(0, 2, 3, 1)\n",
    "        dX = np.zeros(self.X.shape)\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + out_width\n",
    "            dX[:, :, width:width_max] += dcol[:, :, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[16. 22.]\n",
      "  [17. 23.]\n",
      "  [18. 24.]]]\n",
      "[21 41 61]\n",
      "[[[ 32.  53.  74.]\n",
      "  [ 53.  74.  95.]]\n",
      "\n",
      " [[ 62. 103. 144.]\n",
      "  [103. 144. 185.]]\n",
      "\n",
      " [[ 92. 153. 214.]\n",
      "  [153. 214. 275.]]]\n",
      "[[[ 60. 123. 123.  63.]\n",
      "  [ 60. 123. 123.  63.]]]\n"
     ]
    }
   ],
   "source": [
    "test_filter_info = (5, 700)\n",
    "test_initializer = HeInitializer()\n",
    "test_optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "test_conv = Conv1d_test(test_filter_info, test_initializer, test_optimizer)\n",
    "out = test_conv.forward(np.array([[[1, 2, 3, 4], [2, 3, 4, 5]]]))\n",
    "print(out)\n",
    "dout = test_conv.backward(np.array([[[10, 11],[20, 21],[30, 31]]]))\n",
    "print(test_conv.db)\n",
    "print(test_conv.dW)\n",
    "print(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "[numpy.pad — NumPy v1.17 Manual](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializerについて、チャンネルの概念が増えたため対応\n",
    "- Conv1dのコンストラクタ引数にpadを追加\n",
    "- Conv1dをリファクタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer_conv:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    def W(self, filter_info):\n",
    "        filter_num, channel_in, filtersize = filter_info\n",
    "        self.sigma = 1 / np.sqrt(filter_num)\n",
    "        W = self.sigma * np.random.randn(filter_num, channel_in, filtersize)\n",
    "        return W\n",
    "    def B(self, filter_info):\n",
    "        filter_num, channel_in, filtersize = filter_info\n",
    "        B = np.zeros(filter_num)\n",
    "        return B\n",
    "\n",
    "\n",
    "class HeInitializer_conv:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    def W(self, filter_info):\n",
    "        filter_num, channel_in, filtersize = filter_info\n",
    "        self.sigma = np.sqrt(2 / filter_num)\n",
    "        W = self.sigma * np.random.randn(filter_num, channel_in, filtersize)\n",
    "        return W\n",
    "    def B(self, filter_info):\n",
    "        filter_num, channel_in, filtersize = filter_info\n",
    "        B = np.zeros(filter_num)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, filter_info, initializer, optimizer, pad=0):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_info)\n",
    "        self.b = initializer.B(filter_info)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.pad = pad\n",
    "        self.out_width = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        X = X.reshape(X.shape[0], channel_in, X.shape[-1])\n",
    "        self.out_width = X.shape[-1] + 2*self.pad - filtersize + 1\n",
    "        X = np.pad(X, [(0,0), (0,0), (self.pad,self.pad)], 'constant')\n",
    "        self.X = X\n",
    "        col = np.zeros((X.shape[0], channel_in, filtersize, self.out_width))\n",
    "        \n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.out_width\n",
    "            col[:, :, width, :] = X[:, :, width:width_max]\n",
    "        \n",
    "        col = col.transpose(0, 1, 3, 2).reshape(X.shape[0]*self.out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], self.out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, channel_in, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        dcol = dcol.reshape(self.X.shape[0], self.out_width, channel_in, filtersize).transpose(0, 2, 3, 1)\n",
    "        dX = np.zeros(self.X.shape)\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.out_width\n",
    "            dX[:, :, width:width_max] += dcol[:, :, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 7)\n",
      "(3, 1, 786)\n",
      "(1,)\n",
      "(1, 1, 780)\n"
     ]
    }
   ],
   "source": [
    "filter_info = (1, 1, 780)\n",
    "initializer = HeInitializer_conv()\n",
    "optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "conv = Conv1d(filter_info, initializer, optimizer, pad=1)\n",
    "out = conv.forward(X_train[:3])\n",
    "print(out.shape)\n",
    "dout = conv.backward(np.array([[[-2, -1, -3, -1, 2, 1, 1],[-2, -1, -3, -1, 2, 1, 1],[-2, -1, -3, -1, 2, 1, 1]]]))\n",
    "print(dout.shape)\n",
    "print(conv.db.shape)\n",
    "print(conv.dW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 対応済み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv1dのコンストラクタ引数にstrideを追加\n",
    "- Conv1dをリファクタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    def __init__(self, filter_info, initializer, optimizer, pad=0, stride=1):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_info)\n",
    "        self.b = initializer.B(filter_info)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.pad = pad\n",
    "        self.out_width = None\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        X = X.reshape(X.shape[0], channel_in, X.shape[-1])\n",
    "        self.out_width = (X.shape[-1] + 2*self.pad - filtersize)//self.stride + 1\n",
    "        X = np.pad(X, [(0,0), (0,0), (self.pad,self.pad)], 'constant')\n",
    "        self.X = X\n",
    "        col = np.zeros((X.shape[0], channel_in, filtersize, self.out_width))\n",
    "        \n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.stride*self.out_width\n",
    "            col[:, :, width, :] = X[:, :, width:width_max:self.stride]\n",
    "        \n",
    "        col = col.transpose(0, 1, 3, 2).reshape(X.shape[0]*self.out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], self.out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, channel_in, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        dcol = dcol.reshape(self.X.shape[0], self.out_width, channel_in, filtersize).transpose(0, 2, 3, 1)\n",
    "        dX = np.zeros((self.X.shape[0], self.X.shape[1], self.X.shape[2] + self.stride - 1))\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.stride*self.out_width\n",
    "            dX[:, :, width:width_max:self.stride] += dcol[:, :, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 5)\n",
      "(3, 1, 789)\n",
      "(1,)\n",
      "(1, 1, 780)\n"
     ]
    }
   ],
   "source": [
    "filter_info = (1, 1, 780)\n",
    "initializer = HeInitializer_conv()\n",
    "optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "conv = Conv1d(filter_info, initializer, optimizer, pad=2, stride=2)\n",
    "out = conv.forward(X_train[:3])\n",
    "print(out.shape)\n",
    "dout = conv.backward(np.array([[[-2, -1, -3, -1, 1],[-2, -1, -3, -1, 1],[-2, -1, -3, -1, 1]]]))\n",
    "print(dout.shape)\n",
    "print(conv.db.shape)\n",
    "print(conv.dW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_test:\n",
    "    def __init__(self, filter_info, initializer, optimizer, pad=0, stride=1):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = np.ones((3, 2, 3)).astype(float)\n",
    "        self.b = np.array([1, 2, 3]).astype(float)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.pad = pad\n",
    "        self.out_width = None\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        X = X.reshape(X.shape[0], channel_in, X.shape[-1])\n",
    "        self.out_width = (X.shape[-1] + 2*self.pad - filtersize)//self.stride + 1\n",
    "        X = np.pad(X, [(0,0), (0,0), (self.pad,self.pad)], 'constant')\n",
    "        self.X = X\n",
    "        col = np.zeros((X.shape[0], channel_in, filtersize, self.out_width))\n",
    "        \n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.stride*self.out_width\n",
    "            col[:, :, width, :] = X[:, :, width:width_max:self.stride]\n",
    "        \n",
    "        col = col.transpose(0, 1, 3, 2).reshape(X.shape[0]*self.out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], self.out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, channel_in, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        dcol = dcol.reshape(self.X.shape[0], self.out_width, channel_in, filtersize).transpose(0, 2, 3, 1)\n",
    "        dX = np.zeros((self.X.shape[0], self.X.shape[1], self.X.shape[2] + self.stride - 1))\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.stride*self.out_width\n",
    "            dX[:, :, width:width_max:self.stride] += dcol[:, :, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[16. 22.]\n",
      "  [17. 23.]\n",
      "  [18. 24.]]]\n",
      "[21 41 61]\n",
      "[[[ 32.  53.  74.]\n",
      "  [ 53.  74.  95.]]\n",
      "\n",
      " [[ 62. 103. 144.]\n",
      "  [103. 144. 185.]]\n",
      "\n",
      " [[ 92. 153. 214.]\n",
      "  [153. 214. 275.]]]\n",
      "[[[ 60. 123. 123.  63.]\n",
      "  [ 60. 123. 123.  63.]]]\n"
     ]
    }
   ],
   "source": [
    "test_optimizer = AdaGrad(lr=0.01)\n",
    "\n",
    "test_conv = Conv1d_test(test_filter_info, test_initializer, test_optimizer, pad=0, stride=1)\n",
    "out = test_conv.forward(np.array([[[1, 2, 3, 4], [2, 3, 4, 5]]]))\n",
    "print(out)\n",
    "dout = test_conv.backward(np.array([[[10,11],[20,21],[30,31]]]))\n",
    "print(test_conv.db)\n",
    "print(test_conv.dW)\n",
    "print(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】学習と推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sprint_10よりFC, Relu, Softmax, ScratchDeepNeuralNetrowkClassifier, XavierInitializer, HeInitializerを流用\n",
    "- ScratchDeepNeuralNetrowkClassifierをリファクタリング\n",
    "- Conv1dに1次元専用処理を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        out = np.dot(X, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        batch_size = dA.shape[0]\n",
    "        dX = np.dot(dA, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
    "        self.db = np.sum(dA, axis=0) / batch_size\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x < 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def _softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def _cross_entropy_error(self, y, t):\n",
    "        batch_size = y.shape[0]\n",
    "        \n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self._softmax(X)\n",
    "        self.loss = self._cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    \n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "\n",
    "\n",
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "\n",
    "    \n",
    "from collections import OrderedDict\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    def __init__(self, lr, verbose=True, batch_size=20, max_iter=3):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.list_train_loss = []\n",
    "        self.list_test_loss = []\n",
    "        # レイヤの生成\n",
    "        initializer = XavierInitializer()\n",
    "        initializer_conv = XavierInitializer_conv()\n",
    "        optimizer = SGD(lr=lr)\n",
    "        self.layers = OrderedDict()\n",
    "        filter_info = (1, 1, 685)\n",
    "        self.layers[\"Conv1d\"] = Conv1d(filter_info, initializer_conv, optimizer, pad=0, stride=1)\n",
    "        self.layers[\"ReLU\"] = Relu()\n",
    "        self.layers[\"FC\"] = FC(100, 10, initializer, optimizer)\n",
    "        self.lastLayer = Softmax()\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        flg_test = 0\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            flg_test = 1\n",
    "\n",
    "        # 1エポックの繰り返し数\n",
    "        iter_num = int(len(X) / self.batch_size)\n",
    "            \n",
    "        # エポックを複数回繰り返す\n",
    "        for i_ in range(self.max_iter):\n",
    "            # 損失計算用\n",
    "            tmp_list_loss_train = []\n",
    "            tmp_list_loss_val = []\n",
    "\n",
    "            # 1エポック\n",
    "            for j_ in range(iter_num):\n",
    "                batch_mask = np.random.choice(X.shape[0], self.batch_size)\n",
    "                batch_mask_val = np.random.choice(X_val.shape[0], self.batch_size)\n",
    "                X_batch = X[batch_mask]\n",
    "                y_batch = y[batch_mask]\n",
    "                X_val_batch = X_val[batch_mask_val]\n",
    "                y_val_batch = y_val[batch_mask_val]\n",
    "                \n",
    "                self._gradient(X_batch, y_batch)\n",
    "                \n",
    "                loss_train = self._loss(X_batch, y_batch)\n",
    "                tmp_list_loss_train.append(loss_train)\n",
    "                if flg_test == 1:\n",
    "                    loss_test = self._loss(X_val_batch, y_val_batch)\n",
    "                    tmp_list_loss_val.append(loss_test)\n",
    "                    \n",
    "                if self.verbose:\n",
    "                    #verboseをTrueにした際は学習過程などを出力する\n",
    "                    print(\"loss_train:{}\".format(loss_train))\n",
    "                    print(\"loss_test:{}\".format(loss_test))\n",
    "            \n",
    "            # 損失をインスタンス領域に設定\n",
    "            self.list_train_loss.append(sum(tmp_list_loss_train)/len(tmp_list_loss_train))\n",
    "            self.list_test_loss.append(sum(tmp_list_loss_val)/len(tmp_list_loss_val))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        pred = np.argmax(X, axis=1)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def _loss(self, X, t):\n",
    "        for layer in self.layers.values():\n",
    "            X = layer.forward(X)\n",
    "\n",
    "        return self.lastLayer.forward(X, t)\n",
    "\n",
    "    def _gradient(self, X, t):\n",
    "        self._loss(X, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "                    \n",
    "        return\n",
    "    \n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, filter_info, initializer, optimizer, pad=0, stride=1):\n",
    "        self.optimizer = deepcopy(optimizer)\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_info)\n",
    "        self.b = initializer.B(filter_info)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.pad = pad\n",
    "        self.out_width = None\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        X = X.reshape(X.shape[0], channel_in, X.shape[-1])\n",
    "        self.out_width = (X.shape[-1] + 2*self.pad - filtersize)//self.stride + 1\n",
    "        X = np.pad(X, [(0,0), (0,0), (self.pad,self.pad)], 'constant')\n",
    "        self.X = X\n",
    "        col = np.zeros((X.shape[0], channel_in, filtersize, self.out_width))\n",
    "        \n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.stride*self.out_width\n",
    "            col[:, :, width, :] = X[:, :, width:width_max:self.stride]\n",
    "        \n",
    "        col = col.transpose(0, 1, 3, 2).reshape(X.shape[0]*self.out_width, -1)\n",
    "        self.col = col\n",
    "        \n",
    "        # colと計算できるように変形\n",
    "        col_W = self.W.reshape(filter_num, -1).T\n",
    "        self.col_W = col_W\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        # (サンプル数, フィルターと畳み込んだ要素)の形にする\n",
    "        out = out.reshape(X.shape[0], self.out_width, -1).transpose(0, 2, 1)\n",
    "        \n",
    "        # 課題用\n",
    "        out = out.reshape(X.shape[0], -1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        # 課題用\n",
    "        dA = dA.reshape(dA.shape[0], 1, dA.shape[1])\n",
    "        \n",
    "        filter_num, channel_in, filtersize = self.W.shape\n",
    "        dA = dA.transpose(0, 2, 1).reshape(-1, filter_num)\n",
    "        self.db = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(filter_num, channel_in, filtersize)\n",
    "        \n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        # フィルターと内積を取った全ての要素を元イメージの形に変換\n",
    "        dcol = dcol.reshape(self.X.shape[0], self.out_width, channel_in, filtersize).transpose(0, 2, 3, 1)\n",
    "        dX = np.zeros((self.X.shape[0], self.X.shape[1], self.X.shape[2] + self.stride - 1))\n",
    "        for width in range(filtersize):\n",
    "            width_max = width + self.stride*self.out_width\n",
    "            dX[:, :, width:width_max:self.stride] += dcol[:, :, width, :]\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ScratchDeepNeuralNetrowkClassifier(lr=0.1, verbose=False, batch_size=20, max_iter=10)\n",
    "nn.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率:0.8718333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "y_pred = nn.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_val)\n",
    "print(\"正答率:{}\".format(accuracy))\n",
    "\n",
    "x = np.arange(1, len(nn.list_train_loss)+1)\n",
    "plt.plot(x, nn.list_train_loss, label=\"loss\")\n",
    "plt.plot(x, nn.list_test_loss, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUで実行しようとしたところ、メモリ不足でエラーになった。扱うデータは同じだが畳込みの部分で一時的に扱うデータ量を増やしているので、その影響が出たと思われるが今は放置。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
